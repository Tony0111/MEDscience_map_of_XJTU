import pandas as pd
import glob
import os
import matplotlib.pyplot as plt
# =====================================================================
# é¦–å…ˆï¼Œå®šä¹‰æˆ‘ä»¬çš„â€œé¢†åŸŸåœç”¨è¯â€åˆ—è¡¨
# =====================================================================
'''
åœç”¨è¯ä¸»è¦ç”¨äºè¿‡æ»¤æ‰é‚£äº›è¿‡äºé€šç”¨ã€é¢‘ç¹å‡ºç°ä½†å¯¹å…·ä½“ç ”ç©¶é¢†åŸŸæ²¡æœ‰å®é™…æ„ä¹‰çš„å…³é”®è¯ã€‚
è¿™äº›è¯æ±‡å¾€å¾€æ˜¯æè¿°æ€§æˆ–èƒŒæ™¯æ€§çš„ï¼Œä¸èƒ½å¸®åŠ©æˆ‘ä»¬è¯†åˆ«å‡ºçœŸæ­£çš„ç ”ç©¶çƒ­ç‚¹æˆ–è¶‹åŠ¿ã€‚
é€šè¿‡å»é™¤è¿™äº›åœç”¨è¯ï¼Œæˆ‘ä»¬å¯ä»¥æ›´æ¸…æ™°åœ°èšç„¦äºé‚£äº›çœŸæ­£åæ˜ ç‰¹å®šç ”ç©¶é¢†åŸŸçš„å…³é”®è¯ï¼Œä»è€Œæé«˜åˆ†æçš„å‡†ç¡®æ€§å’Œæœ‰æ•ˆæ€§ã€‚
'''
domain_stop_words = [
    # ============== å¸¸è§é€šç”¨ç”Ÿç‰©ä½“/äººç¾¤æ ‡ç­¾ ==============
    'Humans', 'Animals', 'human', 'animal',
    'Female', 'female', 'Male', 'male',
    'Middle Aged',           # ä¸­å¹´äºº
    'Adult',                 # æˆå¹´äºº
    'Aged',                  # è€å¹´äºº
    'Young Adult',           # é’å¹´äºº
    'Adolescent',            # é’å°‘å¹´
    'Child',                 # å„¿ç«¥
    'Aged, 80 and over',     # 80å²åŠä»¥ä¸Šè€å¹´äºº
    "Infant",                # å©´å„¿
    "Child, Preschool",      # å­¦é¾„å‰å„¿ç«¥
    "Infant, Newborn",       # æ–°ç”Ÿå„¿
    "East Asian People",     # ä¸œäºšäºº (é€šå¸¸äººç¾¤å®šä¹‰ä¸ä½œä¸ºçƒ­ç‚¹)

    # ============== å¸¸è§ç ”ç©¶ç±»å‹/æŠ¥å‘Šå½¢å¼/æ–¹æ³•å­¦æ ‡ç­¾ ==============
    'Case Reports', 'patient', 'patients',
    'Retrospective Studies', 'Cross-Sectional Studies', 'Prospective Studies', # ç ”ç©¶ç±»å‹
    'Surveys and Questionnaires', # è°ƒæŸ¥é—®å·
    'Case-Control Studies',       # ç—…ä¾‹å¯¹ç…§ç ”ç©¶
    "Cohort Studies",             # é˜Ÿåˆ—ç ”ç©¶
    "Longitudinal Studies",       # çºµå‘ç ”ç©¶
    "Follow-Up Studies",          # éšè®¿ç ”ç©¶
    "Reproducibility of Results", # ç»“æœé‡ç°æ€§ (ç§‘å­¦ç ”ç©¶çš„åŸºæœ¬è¦æ±‚)
    "Nutrition Surveys",          # è¥å…»è°ƒæŸ¥ (å±äºæ–¹æ³•å­¦ï¼Œè€Œéå…·ä½“ç ”ç©¶å†…å®¹)
    "ROC Curve",                  # ROC æ›²çº¿ (ç»Ÿè®¡å­¦è¯„ä¼°å·¥å…·)
    "meta-analysis",              # Metaåˆ†æ (ç»Ÿè®¡å­¦ç ”ç©¶æ–¹æ³•)
    "Xenograft Model Antitumor Assays", # å¼‚ç§ç§»æ¤æ¨¡å‹æŠ—è‚¿ç˜¤è¯•éªŒ (å…·ä½“çš„å®éªŒæ¨¡å‹å’Œæ–¹æ³•)
    "Proportional Hazards Models",# æ–°å¢ï¼šæ¯”ä¾‹é£é™©æ¨¡å‹ (ç»Ÿè®¡å­¦æ–¹æ³•)

    # ============== å¸¸è§èµ„é‡‘/æœºæ„/åœ°ç†ä½ç½®æ ‡ç­¾ ==============
    'Research Support, N.I.H., Extramural', # åŸºé‡‘æˆ–æœºæ„æ ‡ç­¾
    'United States', # åœ°ç†ä½ç½®
    'United States/epidemiology', # åœ°ç†ä½ç½®/æµè¡Œç—…å­¦
    'China','China/epidemiology', # åœ°ç†ä½ç½®
    "Cities",                     # åŸå¸‚ (è¿‡äºé€šç”¨ï¼Œé€šå¸¸åªä½œä¸ºèƒŒæ™¯)
    "NHANES",                     # ç‰¹å®šç ”ç©¶æ•°æ®é›†

    # ============== å¸¸è§ç ”ç©¶å¯¹è±¡/æ¨¡å‹/å·¥å…·æ ‡ç­¾ ==============
    'Mice', 'Rats',   # ç ”ç©¶å¯¹è±¡ï¼šå°é¼ ã€å¤§é¼ 
    'Mice, Inbred C57BL', # C57BLè¿‘äº¤ç³»å°é¼ 
    'Rats, Sprague-Dawley', # æ–¯æ™®æ‹‰æ ¼-é“åˆ©å¤§é¼ 
    'Mice, Nude',             # è£¸é¼ 
    "Mice, Inbred BALB C",    # BALB/c è¿‘äº¤ç³»å°é¼ 
    "Mice, Knockout",         # åŸºå› æ•²é™¤å°é¼  (ç‰¹å®šç±»å‹åŠ¨ç‰©æ¨¡å‹)
    'Cell Line, Tumor', # è‚¿ç˜¤ç»†èƒç³»
    'Cells, Cultured',        # åŸ¹å…»ç»†èƒ
    "Cell Line",              # ç»†èƒç³» (æ›´é€šç”¨çš„æ¦‚å¿µ)

    # ============== å¸¸è§ã€å®½æ³›çš„ç ”ç©¶æŒ‡æ ‡/æ¦‚å¿µ/è¿‡ç¨‹æ ‡ç­¾ ==============
    'Prognosis',            # ç ”ç©¶ç»“æœï¼šé¢„å
    'prognosis',            # å°å†™é¢„å
    'Risk Factors',         # ç ”ç©¶æ¦‚å¿µï¼šå±é™©å› ç´ 
    'Treatment Outcome',    # ç ”ç©¶ç»“æœï¼šæ²»ç–—ç»“æœ
    'Incidence',            # æµè¡Œç—…å­¦æŒ‡æ ‡ï¼šå‘ç—…ç‡
    "Prevalence",           # æµè¡Œç—…å­¦æŒ‡æ ‡ï¼šæ‚£ç—…ç‡
    'Pregnancy', # å¦‚æœæ˜¯å¦‡äº§ç§‘ï¼Œå¯èƒ½éœ€è¦å»æ‰
    'Apoptosis', # ç»†èƒå‡‹äº¡ï¼ˆå¦‚æœå…¶ç»†åˆ†å¦‚ "/drug effects" æˆ– "/genetics" æ›´é‡è¦ï¼Œåˆ™å¯ä½œä¸ºåœç”¨è¯ï¼‰
    'apoptosis', # å°å†™ç»†èƒå‡‹äº¡
    'Disease Models, Animal', # åŠ¨ç‰©ç–¾ç—…æ¨¡å‹ï¼ˆé€šç”¨æ¦‚å¿µï¼‰
    'Signal Transduction',    # ä¿¡å·è½¬å¯¼ï¼ˆå¦‚æœå…¶ç»†åˆ†å¦‚ "/drug effects" æ›´é‡è¦ï¼Œåˆ™å¯ä½œä¸ºåœç”¨è¯ï¼‰
    'Cell Proliferation',     # ç»†èƒå¢æ®–ï¼ˆå¦‚æœå…¶ç»†åˆ†å¦‚ "/genetics" æˆ– "/drug effects" æ›´é‡è¦ï¼Œåˆ™å¯ä½œä¸ºåœç”¨è¯ï¼‰
    "Inflammation",           # ç‚ç—‡ï¼ˆä½œä¸ºéå¸¸å®½æ³›çš„ç—…ç†è¿‡ç¨‹ï¼Œå¦‚æœæƒ³èšç„¦æ›´å…·ä½“çš„ï¼Œå¯è€ƒè™‘åœç”¨ï¼‰
    "inflammation",           # å°å†™ç‚ç—‡
    "Disease Progression",    # ç–¾ç—…è¿›å±• (é€šç”¨æ€§è´¨æè¿°)
    "Time Factors",           # æ—¶é—´å› ç´  (è¿‡äºæ³›åŒ–)
    "Risk Assessment",        # é£é™©è¯„ä¼° (é€šç”¨ç®¡ç†æ¦‚å¿µ)
    "Quality of Life",        # ç”Ÿæ´»è´¨é‡ (é€šç”¨å¥åº·ç»“å±€æŒ‡æ ‡)
    "Temperature",            # æ¸©åº¦ (é™¤éç‰¹å®šé¢†åŸŸæ‰æœ‰æ„ä¹‰)
    "body mass index",        # èº«ä½“è´¨é‡æŒ‡æ•°
    "Body Mass Index",        # èº«ä½“è´¨é‡æŒ‡æ•°
    "oxidative stress",       # å°å†™æ°§åŒ–åº”æ¿€ (ä¿ç•™å¤§å†™å½¢å¼ä¸ºçƒ­ç‚¹)
    "Genotype",               # åŸºå› å‹ (åŸºç¡€æ¦‚å¿µ)
    "Catalysis",              # å‚¬åŒ– (åŸºç¡€æ¦‚å¿µ)
    "Phenotype",              # è¡¨å‹ (åŸºç¡€æ¦‚å¿µ)

    # ============== åœ¨æ­¤åˆ—è¡¨ä¸­å¤„ç†å¤§å°å†™ä¸ä¸€è‡´çš„è¯æ±‡ ==============
    # è¿™äº›è¯æ±‡æˆ‘ä»¬ä¼šä¿ç•™å…¶æœ‰å¤§å†™æˆ–æ›´å…·ä½“å½¢å¼åœ¨åç»­åˆ†æä¸­ï¼Œæ­¤å¤„åªä¸ºå¤„ç†å°å†™é‡å¤
    "machine learning",
    "immunotherapy"
]
# ä½ å¯ä»¥æ ¹æ®ä½ çš„æ•°æ®ä¸æ–­å®Œå–„è¿™ä¸ªåˆ—è¡¨ï¼


# å®šä¹‰æ•°æ®æ–‡ä»¶è·¯å¾„
data_path = 'D:/MEDscience_map_of_XJTU/data/raw'
# è¯»å–æ•°æ®æ–‡ä»¶
df = pd.read_csv(os.path.join(data_path, 'zotero_data_combined.csv'))


# å®šä¹‰æˆ‘ä»¬è¦ä¿ç•™çš„åˆ—å’Œå®ƒä»¬çš„æ–°åå­—
# è¿™æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œkeyæ˜¯åŸå§‹åˆ—åï¼Œvalueæ˜¯æˆ‘ä»¬çš„æ–°åˆ—å
columns_to_keep = {
    'Author': 'authors',
    'Title': 'title',
    'Publication Title': 'journal',
    'Publication Year': 'year',
    'DOI': 'doi',
    'Abstract Note': 'abstract',
    'Manual Tags': 'manual_tags',
    'Automatic Tags': 'auto_tags',
    'Pages': 'pages',
    'Volume': 'volume',
    'Issue': 'issue'
}
# ä»åŸå§‹DataFrameä¸­åªé€‰æ‹©æˆ‘ä»¬éœ€è¦çš„åˆ—
# æ³¨æ„ï¼šdf[list(columns_to_keep.keys())] ä¼šé€‰æ‹©å‡ºæ‰€æœ‰æˆ‘ä»¬å®šä¹‰å¥½çš„åŸå§‹åˆ—
df_clean = df[list(columns_to_keep.keys())].copy()
# å¯¹é€‰å‡ºçš„æ–°DataFrameè¿›è¡Œé‡å‘½å
df_clean = df_clean.rename(columns=columns_to_keep)
# --- éªŒè¯æˆ‘ä»¬çš„æˆæœ ---
print("æ¸…æ´—åçš„æ•°æ®ä¿¡æ¯ï¼š")
df_clean.info()
print("\næ¸…æ´—åçš„æ•°æ®å‰5è¡Œé¢„è§ˆï¼š")
print(df_clean.head())

# æ ¹æ®info()è¾“å‡ºçš„ä¿¡æ¯å‘ç°ï¼šauto_tagsåˆ—å®Œå…¨ä¸ºç©ºï¼Œå› æ­¤æˆ‘ä»¬å†³å®šä¸¢å¼ƒè¿™ä¸€åˆ—
# ä¸¢å¼ƒ 'auto_tags' åˆ—
# inplace=True è¡¨ç¤ºç›´æ¥åœ¨åŸå§‹çš„ df_clean ä¸Šä¿®æ”¹ï¼Œè€Œä¸æ˜¯è¿”å›ä¸€ä¸ªæ–°DataFrame
df_clean.drop(columns=['auto_tags'], inplace=True)
print("å·²ä¸¢å¼ƒ 'auto_tags' åˆ—ã€‚")

# ä» info() çœ‹å‡ºï¼Œjournal, doi, abstract, manual_tags ç­‰åˆ—éƒ½æœ‰ç¼ºå¤±å€¼ã€‚å¯¹äºè¿™äº›æœ¬åº”æ˜¯å­—ç¬¦ä¸²çš„åˆ—ï¼Œæœ€å¥½çš„å¤„ç†æ–¹æ³•æ˜¯ ç”¨ä¸€ä¸ªç©ºå­—ç¬¦ä¸² '' æ¥å¡«å…… NaNã€‚
# æ‰¾å‡ºæ‰€æœ‰æ•°æ®ç±»å‹ä¸º 'object' (é€šå¸¸æ˜¯å­—ç¬¦ä¸²) çš„åˆ—
string_columns = df_clean.select_dtypes(include=['object']).columns
# ä½¿ç”¨ç©ºå­—ç¬¦ä¸² '' å¡«å……è¿™äº›åˆ—ä¸­çš„æ‰€æœ‰ NaN å€¼
df_clean[string_columns] = df_clean[string_columns].fillna('')
# --- éªŒè¯æˆ‘ä»¬çš„æˆæœ ---
print("\nå¡«å……NaNåçš„æ•°æ®ä¿¡æ¯ï¼š")
df_clean.info()

# 1. å¤„ç†æœ‰DOIçš„è®°å½•
#   - isin(['']) æ˜¯ä¸ºäº†ç¡®ä¿æˆ‘ä»¬é€‰ä¸­äº†ç©ºå­—ç¬¦ä¸²
#   - ~ ç¬¦å·æ˜¯ "å–å" çš„æ„æ€ï¼Œæ‰€ä»¥è¿™é‡Œæ˜¯é€‰æ‹© doi ä¸æ˜¯ç©ºå­—ç¬¦ä¸²çš„è¡Œ
df_has_doi = df_clean[~df_clean['doi'].isin([''])].copy()
df_no_doi = df_clean[df_clean['doi'].isin([''])].copy()
# å¯¹æœ‰DOIçš„éƒ¨åˆ†è¿›è¡Œå»é‡
print(f"å»é‡å‰ï¼Œæœ‰DOIçš„è®°å½•æ•°: {len(df_has_doi)}")
df_has_doi.drop_duplicates(subset=['doi'], keep='first', inplace=True)
print(f"å»é‡åï¼Œæœ‰DOIçš„è®°å½•æ•°: {len(df_has_doi)}")

# 2. å¤„ç†æ²¡æœ‰DOIçš„è®°å½•
#    å¯¹äºæ²¡æœ‰DOIçš„ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªæ›´ä¸¥æ ¼çš„æ ‡å‡†ï¼šæ ‡é¢˜ã€å¹´ä»½å’Œä½œè€…åˆ—è¡¨éƒ½å¾—ä¸€æ ·æ‰ç®—é‡å¤
print(f"\nå»é‡å‰ï¼Œæ— DOIçš„è®°å½•æ•°: {len(df_no_doi)}")
df_no_doi.drop_duplicates(subset=['title', 'year', 'authors'], keep='first', inplace=True)
print(f"å»é‡åï¼Œæ— DOIçš„è®°å½•æ•°: {len(df_no_doi)}")

# 3. å°†å¤„ç†å¥½çš„ä¸¤éƒ¨åˆ†æ•°æ®é‡æ–°åˆå¹¶èµ·æ¥
df_final = pd.concat([df_has_doi, df_no_doi], ignore_index=True)
# --- æœ€ç»ˆéªŒè¯ ---
print(f"\næ‰€æœ‰æ•°æ®å»é‡å®Œæˆï¼æœ€ç»ˆå‰©ä½™ {len(df_final)} æ¡æœ‰æ•ˆè®°å½•ã€‚")
print("\næœ€ç»ˆæ•°æ®é›†ä¿¡æ¯ï¼š")
df_final.info()
# ç°åœ¨ï¼Œdf_clean å¯ä»¥è¢« df_final è¦†ç›–ï¼Œæˆ–è€…ä½ åé¢ä¸€ç›´ä½¿ç”¨ df_final
df_clean = df_final

def standardize_author_name(name):
    """
    å°†å•ä¸ªä½œè€…å§“åæ ‡å‡†åŒ–ä¸º 'Lastname F.' çš„æ ¼å¼ã€‚
    ä¾‹å¦‚ï¼š'Jin, Xuting' -> 'Jin X'
           'Xuting Jin' -> 'Jin X'
    """
    name = name.strip()  # å»é™¤é¦–å°¾çš„ç©ºæ ¼
    if not name:
        return None  # å¦‚æœæ˜¯ç©ºå­—ç¬¦ä¸²ï¼Œè¿”å›None

    # æƒ…å†µ1: 'Lastname, Firstname' æ ¼å¼
    if ',' in name:
        parts = name.split(',', 1) # åªåˆ†å‰²ä¸€æ¬¡
        last_name = parts[0].strip()
        first_name = parts[1].strip()
        if first_name:
            return f"{last_name} {first_name[0]}"
        else:
            return last_name # åªæœ‰å§“çš„æƒ…å†µ
    
    # æƒ…å†µ2: 'Firstname Lastname' æ ¼å¼
    else:
        parts = name.split()
        if len(parts) > 1:
            last_name = parts[-1].strip()
            first_name = parts[0].strip()
            return f"{last_name} {first_name[0]}"
        else:
            # åªæœ‰ä¸€ä¸ªè¯çš„åå­— (æ¯”å¦‚ä¸€ä¸ªæœºæ„åè¯¯å…¥)
            return name
        
        # df_clean æ˜¯ä½ ä¸Šä¸€æ­¥å¾—åˆ°çš„æœ€ç»ˆDataFrame
def process_authors_string(authors_str):
    """
    å¤„ç†æ•´ä¸ªä½œè€…å­—ç¬¦ä¸²ï¼Œè¿”å›ä¸€ä¸ªæ ‡å‡†åŒ–çš„ä½œè€…åˆ—è¡¨ã€‚
    """
    if not authors_str or pd.isna(authors_str):
        return [] # å¦‚æœæ˜¯ç©ºå­—ç¬¦ä¸²æˆ–NaNï¼Œè¿”å›ç©ºåˆ—è¡¨

    authors_list_raw = authors_str.split(';')
    
    # å¯¹åˆ—è¡¨ä¸­çš„æ¯ä¸ªåå­—åº”ç”¨æ ‡å‡†åŒ–å‡½æ•°
    standardized_list = [standardize_author_name(name) for name in authors_list_raw]
    
    # è¿‡æ»¤æ‰å¤„ç†å¤±è´¥çš„ None å€¼
    return [name for name in standardized_list if name is not None]

# ä½¿ç”¨ .apply() æ–¹æ³•å°†è¿™ä¸ªå‡½æ•°åº”ç”¨åˆ° 'authors' åˆ—çš„æ¯ä¸€è¡Œ
df_clean['authors_list'] = df_clean['authors'].apply(process_authors_string)

# --- éªŒè¯æˆ‘ä»¬çš„æˆæœ ---
# æŸ¥çœ‹æ–°åˆ›å»ºçš„åˆ—å’ŒåŸå§‹åˆ—çš„å¯¹æ¯”
print("\nä½œè€…ä¿¡æ¯å¤„ç†å‰åå¯¹æ¯”ï¼š")
print(df_clean[['authors', 'authors_list']].head())

# æ£€æŸ¥ä¸€ä¸‹æŸä¸€è¡Œçš„æ•°æ®
print("\næŸ¥çœ‹å•è¡Œå¤„ç†ç»“æœç¤ºä¾‹ï¼š")
print("åŸå§‹æ•°æ®:", df_clean.loc[0, 'authors'])
print("å¤„ç†å:", df_clean.loc[0, 'authors_list'])



# ===========================================================================


import pandas as pd
import networkx as nx
from collections import Counter
from itertools import combinations

# =========================================================================
#  å‰æï¼šå‡è®¾ df_clean å·²ç»å­˜åœ¨äºæ‚¨çš„ç¯å¢ƒä¸­
# =========================================================================

# --- é…ç½®å‚æ•° (è¯·åœ¨æ­¤å¤„ä¿®æ”¹) ---
KEYWORD_COLUMN = 'manual_tags'
SEPARATOR = ';' 


# è½¬æ¢ä¸ºå°å†™ã€å»é™¤å‰åç©ºæ ¼ï¼Œå¹¶å­˜å…¥setä¸­ä»¥åŠ å¿«æŸ¥æ‰¾é€Ÿåº¦
domain_stop_words_set = {word.strip().lower() for word in domain_stop_words if word.strip()}
# -------------------------------------------------------------

# 2. è°ƒæ•´è¿‡æ»¤é˜ˆå€¼
MIN_FREQUENCY = 10
MIN_WEIGHT = 5
# =========================================================================


print("--- æ­¥éª¤1ï¼šæå–å’Œå¤„ç†å…³é”®è¯ (å·²é›†æˆåœç”¨è¯) ---")
print(f"åŠ è½½å¹¶æ¸…æ´—äº† {len(domain_stop_words_set)} ä¸ªé¢†åŸŸåœç”¨è¯ã€‚")
df_clean[KEYWORD_COLUMN] = df_clean[KEYWORD_COLUMN].fillna('').astype(str)

clean_tags_list = df_clean[KEYWORD_COLUMN].apply(
    lambda x: [
        tag.strip() for tag in x.lower().split(SEPARATOR)
        if tag.strip() and tag.strip().lower() not in domain_stop_words_set
    ]
).tolist()

all_tags_flat = [tag for sublist in clean_tags_list for tag in sublist]
tag_frequencies = Counter(all_tags_flat)
print(f"å»é™¤åœç”¨è¯åï¼Œæ•°æ®é›†ä¸­å…±æœ‰ {len(tag_frequencies)} ä¸ªç‹¬ç«‹å…³é”®è¯ã€‚")


print("\n--- æ­¥éª¤2ï¼šæ„å»ºå®Œæ•´çš„å…±ç°ç½‘ç»œ ---")
G_full = nx.Graph()
for tags_in_doc in clean_tags_list:
    for pair in combinations(sorted(set(tags_in_doc)), 2):
        if G_full.has_edge(pair[0], pair[1]):
            G_full[pair[0]][pair[1]]['weight'] += 1
        else:
            G_full.add_edge(pair[0], pair[1], weight=1)
print(f"åŸå§‹ç½‘ç»œåŒ…å« {G_full.number_of_nodes()} ä¸ªèŠ‚ç‚¹å’Œ {G_full.number_of_edges()} æ¡è¾¹ã€‚")


print("\n--- æ­¥éª¤3ï¼šè¿›è¡Œæ ¸å¿ƒè¿‡æ»¤ï¼Œä¸ºç½‘ç»œâ€œç˜¦èº«â€ ---")
frequent_keywords = {kw for kw, freq in tag_frequencies.items() if freq >= MIN_FREQUENCY}
G_filtered = nx.Graph()
for u, v, data in G_full.edges(data=True):
    if u in frequent_keywords and v in frequent_keywords and data['weight'] >= MIN_WEIGHT:
        G_filtered.add_edge(u, v, weight=data['weight'])

isolated_nodes = list(nx.isolates(G_filtered))
G_filtered.remove_nodes_from(isolated_nodes)
print("ç½‘ç»œè¿‡æ»¤å®Œæˆï¼")
final_nodes = G_filtered.number_of_nodes()
final_edges = G_filtered.number_of_edges()
print(f"è¿‡æ»¤åçš„ç½‘ç»œåŒ…å« {final_nodes} ä¸ªèŠ‚ç‚¹å’Œ {final_edges} æ¡è¾¹ã€‚")


print("\n--- æ­¥éª¤4ï¼šå¯¼å‡ºè¿‡æ»¤åçš„ç½‘ç»œä¸º VOSviewer æ–‡ä»¶ ---")
G_keywords_weighted = G_filtered
all_keywords = list(G_keywords_weighted.nodes())
if not all_keywords:
    print("\n!! è­¦å‘Šï¼šè¿‡æ»¤åç½‘ç»œä¸ºç©º !! è¯·é™ä½è¿‡æ»¤é˜ˆå€¼ã€‚")
else:
    keyword_to_id = {keyword: i + 1 for i, keyword in enumerate(all_keywords)}

    # åˆ›å»º Map File (éœ€è¦è¡¨å¤´)
    map_filename = 'vosviewer_map_filtered.txt'
    with open(map_filename, 'w', encoding='utf-8') as f:
        f.write("id\tlabel\tweight\n")
        for keyword in all_keywords:
            node_id = keyword_to_id[keyword]
            label = keyword.replace('"', '').replace("'", "")
            weight = tag_frequencies.get(keyword, 1)
            f.write(f"{node_id}\t{label}\t{weight}\n")
    print(f"æˆåŠŸåˆ›å»º Map File: '{map_filename}'")

    # åˆ›å»º Network File (ä¸èƒ½æœ‰è¡¨å¤´)
    network_filename = 'vosviewer_network_filtered.txt'
    with open(network_filename, 'w', encoding='utf-8') as f:
        for u, v, data in G_keywords_weighted.edges(data=True):
            start_id = keyword_to_id[u]
            end_id = keyword_to_id[v]
            link_strength = data['weight']
            f.write(f"{start_id}\t{end_id}\t{link_strength}\n")
    print(f"æˆåŠŸåˆ›å»º Network File: '{network_filename}'")
    
    print("\nğŸ‰ å…¨éƒ¨å®Œæˆï¼åœç”¨è¯é—®é¢˜å·²ä¿®å¤ï¼Œæ–‡ä»¶æ ¼å¼æ­£ç¡®ã€‚")



# =====================================================================
'''python
import pandas as pd
import networkx as nx
from collections import Counter
from itertools import combinations


from collections import Counter
from itertools import combinations
import networkx as nx
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns


import networkx as nx
from itertools import combinations 

import matplotlib.pyplot as plt
import seaborn as sns

import matplotlib.pyplot as plt
import seaborn as sns


import pandas as pd
import glob
import os
'''
